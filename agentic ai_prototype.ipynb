{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import re\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from docx import Document\n",
    "from langchain_core.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up open Ai env variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up vector config tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .docx file and extract the text content\n",
    "config_document = Document(\"RTB Config document compilation.docx\")\n",
    "\n",
    "config_text = \"\"\n",
    "for paragraph in config_document.paragraphs:\n",
    "    config_text += paragraph.text + \"\\n\"\n",
    "\n",
    "# Split the config content into individual documents based on headings (assuming \"##\" as a delimiter)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
    "config_doc_chunks = text_splitter.split_text(config_text)\n",
    "\n",
    "# Initialize OpenAI embeddings for ChromaDB\n",
    "OpenAIembedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")  # Updated model name\n",
    "\n",
    "# Initialize ChromaDB vector store from text and embeddings\n",
    "vector_store = Chroma.from_texts(config_doc_chunks, OpenAIembedding, collection_name=\"config_document\")\n",
    "\n",
    "\n",
    "# VectorStoreRetriever class using ChromaDB\n",
    "class VectorStoreRetriever:\n",
    "    def __init__(self, vector_store: Chroma):\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def query(self, query: str, k: int = 5) -> list[dict]:\n",
    "        # Perform a similarity search in ChromaDB\n",
    "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "        return [\n",
    "            {\n",
    "                \"page_content\": result[0].page_content,  \n",
    "                \"similarity_score\": result[1],\n",
    "                \n",
    "            }\n",
    "            for result in results\n",
    "        ]\n",
    "\n",
    "# Initialize retriever with the vector store\n",
    "retriever = VectorStoreRetriever(vector_store)\n",
    "\n",
    "@tool\n",
    "def lookup_config(query: str) -> str:\n",
    "    \"\"\"Look up relevant parts of the config document based on the query.\"\"\"\n",
    "    docs = retriever.query(query, k=2)\n",
    "    return \"\\n\\n\".join([doc[\"page_content\"] for doc in docs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up method parsing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document = Document(\"Calss_export_word.docx\")\n",
    "document\n",
    "\n",
    "text = \"\"\n",
    "for paragraph in document.paragraphs:\n",
    "    text += paragraph.text + \"\\n\"\n",
    "\n",
    "cleaned_text = text.replace('\\xa0', ' ')\n",
    "\n",
    "\n",
    "method_pattern = r\"(?<!\\w)METHOD\\s+([\\w_]+)\\.\\s*(.*?)\\s*ENDMETHOD\\.\"\n",
    "\n",
    "# Extract methods and contents\n",
    "methods_dict = {name.lower().strip(): content.strip() for name, content in re.findall(method_pattern, cleaned_text, re.DOTALL)}\n",
    "\n",
    "def extract_method_details(text):\n",
    "    method_pattern = r\"class-methods\\s+([^\\s]+)\\s+\"\n",
    "    importing_pattern = r\"importing\\s+(.*?)\\s+(?:returning|exporting)\"\n",
    "    exporting_pattern = r\"exporting\\s+(.*?)\\s*\\.\"\n",
    "    returning_pattern = r\"returning\\s+(.*?)\\s*\\.\"\n",
    "    \n",
    "\n",
    "    method_details = {}\n",
    "    for match in re.finditer(method_pattern, text):\n",
    "        method_name = match.group(1).lower()\n",
    "        method_details[method_name] = {}\n",
    "\n",
    "        importing_match = re.search(importing_pattern, text[match.end():])\n",
    "        if importing_match:\n",
    "            method_details[method_name]['importing'] = [param.strip() for param in importing_match.group(1).split('\\n') if param.strip()]\n",
    "\n",
    "        returning_match = re.search(returning_pattern, text[match.end():], re.DOTALL)\n",
    "        if returning_match:\n",
    "            method_details[method_name]['exporting'] = [param.strip() for param in returning_match.group(1).split('\\n') if param.strip()]\n",
    "\n",
    "        exporting_match = re.search(exporting_pattern, text[match.end():], re.DOTALL)\n",
    "        if exporting_match:\n",
    "            method_details[method_name]['exporting'] = [param.strip() for param in exporting_match.group(1).split('\\n') if param.strip()]\n",
    "\n",
    "    return method_details\n",
    "\n",
    "method_params = extract_method_details(cleaned_text)\n",
    "print(method_params)\n",
    "\n",
    "\n",
    "consolidated_dict = {}\n",
    "\n",
    "for method_name, method_content in methods_dict.items():\n",
    "    consolidated_dict[method_name] = {\n",
    "        \"method_code\": method_content,\n",
    "        \"importing_parameters\": method_params.get(method_name, {}).get('importing', []),\n",
    "        \"exporting_parameters\": method_params.get(method_name, {}).get('exporting', []),\n",
    "        \"returning_parameters\": method_params.get(method_name, {}).get('returning', []),\n",
    "    }\n",
    "\n",
    "print(consolidated_dict)\n",
    "\n",
    "class_pattern = r\"class\\s+([^\\s]+)\\s+\"\n",
    "class_name_match = re.search(class_pattern, cleaned_text)\n",
    "class_name = class_name_match.group(1).lower() if class_name_match else \"unknown_class\"\n",
    "class_details = {class_name: consolidated_dict}\n",
    "\n",
    "# Tool to parse the class dictionary based on the method name.\n",
    "\n",
    "@tool(\"parse_class_documentation\", return_direct=True)\n",
    "def parse_class_dict(method_name: str):\n",
    "    \"\"\"\n",
    "    Parses the class documentation to extract the method details for a specific class and method.\n",
    "    \n",
    "    Arguments:\n",
    "    - class_name: The class name for which the method belongs.\n",
    "    - method_name: The specific method name for which the documentation is requested.\n",
    "\n",
    "    Returns:\n",
    "    - The method code and parameters as part of the documentation.\n",
    "    \"\"\"\n",
    "    method_name = method_name.lower()\n",
    "    if class_name in class_details and method_name in class_details[class_name]:\n",
    "        method_code = class_details[class_name][method_name]['method_code']\n",
    "        return method_code\n",
    "    return \"Method not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the graph query tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"neo4j+s://909a82f6.databases.neo4j.io\"  # or neo4j+s://xxxx.databases.neo4j.io\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"ERHMOSLUToDtV33RdV3oRpne18Aoie82tOZVqAHl6KE\"  # your password\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# Create a Neo4j driver\n",
    "graph = Neo4jGraph(url=NEO4J_URI , username=NEO4J_USERNAME , password=NEO4J_PASSWORD, enhanced_schema=True)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model = 'gpt-4o',temperature=0)\n",
    "\n",
    "# Connect to Neo4j\n",
    "\n",
    "driver = GraphDatabase.driver(uri=NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "@tool(\"generate_and_run_cypher_query\", return_direct=True)\n",
    "def generate_and_run_cypher_query(question: str):\n",
    "    \"\"\"\n",
    "    Call this tool first.\n",
    "\n",
    "    Tool to generate and execute a Cypher query based on the provided schema and user query.\n",
    "    \n",
    "    Arguments:\n",
    "    - user_query: The user's input question that will be converted into a Cypher query to run on the graph database.\n",
    "\n",
    "    Returns:\n",
    "    - The results of the Cypher query executed on the graph. The results of the cypher query contains dependencies that will be useful in documentation generation\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt for query generation\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"schema\", \"question\"],\n",
    "        template=\"\"\"   \n",
    "        Generate a Cypher query to answer this question: {question}\n",
    "    You are an expert in Neo4j and generating Cypher queries to query a graph database. Follow these instructions closely (non negotiable):\n",
    "\n",
    "    Use only the provided relationship types and properties in the schema.\n",
    "    Do not use any other relationship types or properties that are not provided.\n",
    "\n",
    "    *Ensure case-insensitive name matching by converting both the stored name property and the input string to lowercase using toLower().*\n",
    "\n",
    "    *If the Cypher query requires the following condition:\n",
    "        WHERE toLower(method.name) = toLower(\"bdr_reject\")\n",
    "    Instead, try using:\n",
    "        WHERE toLower(method.name) CONTAINS toLower(\"bdr_reject\")*\n",
    "\n",
    "    The schema includes the following entities: [Class, Method, Function, Variable, Table].\n",
    "\n",
    "    If the exact answer is not found, attempt to use the in_community relationship, which can sometimes provide useful insights.\n",
    "\n",
    "    Do not include any explanations, apologies, or additional text other than the Cypher query itself.\n",
    "    Only generate the Cypher query that answers the given question.\n",
    "\n",
    "    Replace \"entity\" with __Entity__ in all queries to reflect the schema's naming convention.\n",
    "\n",
    "\n",
    "    Schema:\n",
    "    {schema}\n",
    "   \n",
    "    Examples: Here are a few examples of generated Cypher statements for particular questions:\n",
    "    # What are all the methods that are related to billing\n",
    "    MATCH path = (entity:Method )-[r]-(billingcommunity:__Community__)\n",
    "    WHERE toLower(billingcommunity.title) CONTAINS toLower(\"billing\")\n",
    "    RETURN path\n",
    "\n",
    "    #How many methods exist in the class?\n",
    "    MATCH path = (entity:Method)-[:RELATED]-(class:Class)\n",
    "    RETURN count(path)\n",
    "\n",
    "    #Methods with most dependencies?\n",
    "    MATCH (method:Method)-[r]->(__Entity__)\n",
    "    WHERE entity:Class OR entity:Variable OR entity:Table or entity:Method or entity:Function\n",
    "    WITH method, count(r) AS dependenciesCount\n",
    "    RETURN method.name AS MethodName, dependenciesCount\n",
    "    ORDER BY dependenciesCount DESC\n",
    "\n",
    "    #What are the functions present in a method?\n",
    "    MATCH (method:Method)-[:RELATED]-(function:Function)\n",
    "    RETURN method.name AS MethodName, function.name AS FunctionName\n",
    "\n",
    "    MATCH (method:Method)-[:RELATED]-(function:Function)\n",
    "    WHERE toLower(method.name) CONTAINS toLower(\"copy_clgrp_cond\")\n",
    "    RETURN function.name AS FunctionName\n",
    "    full context: [{{'FunctionName': '/CGDC/CLRQ_DATA_GET'}}]\n",
    "    the full context reurned from the query contains the answer to the question\n",
    "    answer = '/CGDC/CLRQ_DATA_GET'\n",
    "\n",
    "    #what are the functions used in a Get_Buffer_conditions method?\n",
    "    MATCH (method:Method)-[:RELATED]-(function:Function)\n",
    "    WHERE toLower(method.name) CONTAINS toLower(\"get_buffer_conditions\")\n",
    "    RETURN function.name AS FunctionName\n",
    "\n",
    "    whenever you use entity in the query replace it with __Entity__ in the query\n",
    "    in the response to questions like \n",
    "\n",
    "    \"what are the entities that are related to SELECT_AMOUNTS_CALC_GRP_ROW \"\n",
    "    MATCH (method:Method)-[:RELATED]-(entity:__Entity__)\n",
    "    WHERE toLower(method.name) CONTAINS toLower(\"copy_clgrp_cond\")\n",
    "    RETURN entity.name\n",
    "  \n",
    "\n",
    "  \n",
    "        \n",
    "        Cypher query:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Prompt for response generation\n",
    "    response_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"query_result\"],\n",
    "        template=\"\"\"\n",
    "        Question: {question}\n",
    "        \n",
    "        Based on the following query result:\n",
    "        {query_result}\n",
    "        \n",
    "        Provide a concise answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def get_schema():\n",
    "        return graph.structured_schema\n",
    "        \n",
    "\n",
    "    def execute_query(query: str):\n",
    "        with driver.session() as session:\n",
    "            pattern = r\"```cypher\\s*(.*?)\\s*```\"\n",
    "\n",
    "            # Find match\n",
    "            match = re.search(pattern, query, re.DOTALL)\n",
    "\n",
    "            # Check if match is found and print the result\n",
    "            if match:\n",
    "                clean_query = match.group(1).strip()  # Group 1 captures everything after 'cypher'\n",
    "                # print(\"Text after 'cypher':\", clean_query)\n",
    "            else:\n",
    "                print(\"No match found\")\n",
    "            result = session.run(clean_query)\n",
    "            return [record.data() for record in result]\n",
    "\n",
    "    def answer_question(question: str)-> str:\n",
    "        question_conversion_prompt = f\"\"\"Extract the SAP method name from the user query: {question}. You can refer to the method names in {method_params.keys()}. \n",
    "        Return the response as find the entities in relation with <<extracted method name>>\"\"\"   \n",
    "        question = llm.invoke(question_conversion_prompt)\n",
    "        print(\"question_after conversion\", question)    \n",
    "        # Generate Cypher query\n",
    "        schema = get_schema()\n",
    "        query = llm.invoke(query_prompt.format(schema=schema, question=question))\n",
    "\n",
    "        # Execute query\n",
    "        result = execute_query(query.content) #the answers recieved by running the cypher query\n",
    "        # print(\"result after eexcute_query\", result)\n",
    "        \n",
    "        # Generate response\n",
    "        response = llm.invoke(response_prompt.format(question=question, query_result=str(result)))\n",
    "        \n",
    "        return response\n",
    "    return answer_question(question).content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the graph query tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What entities are related to COPY_CLGRP_COND method?\"\n",
    "\n",
    "# Generate and run the Cypher query using the dynamic schema\n",
    "result = generate_and_run_cypher_query(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the function dictionary and tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function parsing logic\n",
    "def extract_function_details(text):\n",
    "    # Adjusted regex pattern to match the function name\n",
    "    function_name_pattern = r\"(?<=FUNCTION\\s)(\\/\\w+\\/\\w+)\"\n",
    "    # Adjusted regex pattern to match the entire function block\n",
    "    function_pattern = r\"FUNCTION\\s+([\\w_\\/]+)\\.\\s*(.*?)\\s*ENDFUNCTION\\.\"\n",
    "\n",
    "    # Dictionary to store function name as key and details as value\n",
    "    function_details = {}\n",
    "\n",
    "    # Find all function blocks\n",
    "    for match in re.finditer(function_pattern, text, re.DOTALL):\n",
    "        # Extract function name using the previously defined function_name_pattern\n",
    "        function_name_match = re.search(function_name_pattern, match.group(0))\n",
    "        if function_name_match:\n",
    "            function_name = function_name_match.group(1).lower()  \n",
    "            function_content = match.group(2).strip()  \n",
    "            # Store content as the value\n",
    "            # print(function_name)\n",
    "            # print(function_content)\n",
    "            function_details[function_name] = function_content\n",
    "\n",
    "    return function_details\n",
    "\n",
    "# Wrapping into a LangChain tool\n",
    "@tool(\"parse_function_documentation\", return_direct=True)\n",
    "def parse_function_documentation(function_name: str):\n",
    "    \"\"\"\n",
    "    Parses ABAP function documentation from the provided text.\n",
    "    Extracts function names and their respective content between FUNCTION and ENDFUNCTION.\n",
    "    Helpful in providing details of functions that can be used for understanding a method or class.\n",
    "    \"\"\"\n",
    "    function_name = function_name.lower()\n",
    "    for function_name in function_params.keys():\n",
    "        if function_name in function_params:\n",
    "            return function_params[function_name]\n",
    "\n",
    "    return extract_function_details(function_text)\n",
    "\n",
    "# Example usage of the tool\n",
    "\n",
    "with open('Function clrq_data_get..txt', 'r') as file:\n",
    "    function_text = file.read()\n",
    "\n",
    "function_params = extract_function_details(function_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretty print the messages and log errors\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=15000000):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_method_params():\n",
    "    global method_params\n",
    "    method_params = extract_method_details(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Assistant class\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable, graph=None, method_params=None):\n",
    "        #graph = Neo4jGraph(url=NEO4J_URI , username=NEO4J_USERNAME , password=NEO4J_PASSWORD, enhanced_schema=True)\n",
    "        self.runnable = runnable\n",
    "        #self.graph = graph\n",
    "        self.method_params = method_params\n",
    "\n",
    "    def __call__(self, state: dict, config: RunnableConfig, method_params = method_params):\n",
    "        while True:\n",
    "            print(f\"Configuration received: {config}\")\n",
    "            messages = state.get(\"messages\", [])\n",
    "            print(messages)\n",
    "\n",
    "            # Extract the user query from the messages and send the method name to generate_and_run_cypher_query tool\n",
    "            if \"generate_and_run_cypher_query\" in self.runnable.get_name():\n",
    "                print(\"Graph Query Tool detected.\")\n",
    "                # method_params = self.method_params if self.method_params else {}\n",
    "                # graph = self.graph if self.graph else None  # Ensure graph is set in the state\n",
    "\n",
    "                # Prepare the input for the tool\n",
    "                tool_input = {\n",
    "                                \"question\": question,  # Use the extracted query\n",
    "                                     # Ensure graph is properly passed\n",
    "                          # Ensure schema is properly passed\n",
    "                                \"context\": method_params.keys()  # Pass method params to the tool\n",
    "        }\n",
    "                 # Invoke the runnable with the prepared input\n",
    "                result = self.runnable.invoke(tool_input)\n",
    "                print(\"Result from Graph Query Tool:\", result)\n",
    "            else:\n",
    "                print(\"Invoking other tools or processing states.\")\n",
    "                result = self.runnable.invoke(state)\n",
    "\n",
    "\n",
    "            # Check if LLM returned a valid response or if we need to retry\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Please provide a valid response.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "# Initialize OpenAI's GPT-4 model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# Define a prompt template for the assistant to generate proper queries based on user input\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant focused on understanding SAP ABAP code, \"\n",
    "            \"documenting methods, functions, and resolving queries using the provided tools. \"\n",
    "            \"For every query, use the tools effectively. When searching in the graph or config, \"\n",
    "            \"ensure you understand the context of the class/method/functions.\"\n",
    "            \"Try and identify the method name from the user query.\"\n",
    "            \"If the user is asking to print a documentation, use the generate and run query tool first, to check if functions exist or not.\"\n",
    "            \"Use the output from the parse class documentation to explain what the method or class is doing.\"\n",
    "            \"\"\"When you identify a method is being used, use the function documentation tool to understand the function and create a function documentation. Use the function documentation \n",
    "            to enhance the understanding of the method. and then generate a highly technical documentation by incorporating method and function understanding.\"\"\" \n",
    "            \"\\n\\nUser Query:\\n<User>\\n{{user_query}}\\n</User>\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "# List of tools that will be bound to the assistant (adjusted for your use case)\n",
    "part_1_tools = [\n",
    "    generate_and_run_cypher_query,  # Tool for graph querying\n",
    "    lookup_config,  # Tool for retrieving information from the configuration document\n",
    "    parse_class_dict,  # Tool for extracting class and method information from the doc\n",
    "    parse_function_documentation,  # Tool to parse and understand functions\n",
    "]\n",
    "\n",
    "# Bind the tools to the assistant prompt using LangChain’s tool binding mechanism\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools)\n",
    "\n",
    "# Example of the Assistant node setup using LangGraph (simplified)\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Define nodes: assistant and tools\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\n",
    "\n",
    "# Define edges for control flow\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\"assistant\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "builder.add_edge(\"assistant\", END)\n",
    "\n",
    "# Create the memory to save the graph state\n",
    "memory = MemorySaver()\n",
    "part_1_graph = builder.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "tutorial_questions = [\n",
    "    \"Write a documentation for the method BDR_REJECT\",\n",
    "]\n",
    "\n",
    "# Let's use a UUID for the thread ID for consistent state checkpoints\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "# Configuration, can include things like default class or method names to query\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The thread_id can be used for persistent checkpoints\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "_printed = set()\n",
    "\n",
    "# Iterate over the tutorial questions and simulate the conversation with the agent\n",
    "for question in tutorial_questions:\n",
    "    # Stream the events from the graph, processing the user's question\n",
    "    events = part_1_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    \n",
    "    # Print the responses from the agent for each event\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "tutorial_questions = [\n",
    "    \"Write a documentation for class /CGDC/CL_CLRQ_PROCESS\",\n",
    "]\n",
    "\n",
    "# Let's use a UUID for the thread ID for consistent state checkpoints\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "# Configuration, can include things like default class or method names to query\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The thread_id can be used for persistent checkpoints\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "_printed = set()\n",
    "\n",
    "# Iterate over the tutorial questions and simulate the conversation with the agent\n",
    "for question in tutorial_questions:\n",
    "    # Stream the events from the graph, processing the user's question\n",
    "    events = part_1_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    \n",
    "    # Print the responses from the agent for each event\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
